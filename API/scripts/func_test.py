from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip
from utils.extract_audio import extract_audio
from utils.skilearn_model import *
from model.whisper import Whisper_model
from utils.api_maker import create_dict_data
from vertical_video_maker import *
from utils.lama_make_interval import *
from utils.get_datainfo_llm import *
import os

def func_test(video_path):
    """
    {
        "data": [{desrcibe: "text"
            emoji: "üîç"
            tag: "#test
            source: ML
            number: 1
            transcription: "text"}
            ...
        ],
        "video": [file1, ...],
        "audio": [audio1, ...]

    }
    """
    start_total = time.time()
    if not os.path.exists('short_videos'):
        os.makedirs('short_videos')
    if not os.path.exists('short_audio'):
        os.makedirs('short_audio')
    if not os.path.exists('data_sub'):
        os.makedirs('data_sub')

    path_audio = f"short_audio/{video_path.split('.')[-2].split('/')[-1]}.mp3"
    extract_audio(video_path, path_audio)

    start = time.time()
    # –†–∞–±–æ—Ç–∞ –ú–õ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
    y_test_df, sr = find_anomalies_audio(path_audio)
    intervals, starting_times, = postproccess_annomalies(y_test_df, sr)
    print(intervals)
    end = time.time()
    print(f'–†–∞–±–æ—Ç–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –ú–õ –≤ —Å {end-start}')

    print('–†–∞–±–æ—Ç–∞ –º–æ–¥–µ–ª–∏ –≤–∏—Å–ø–µ—Ä —Å –∑–∞–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–∏')
    start = time.time()
    model = Whisper_model()
    transcribe_video = model.transcribe(f"{path_audio}", language='ru', verbose=False, beam_size=5, best_of=5)
    end = time.time()
    print(f'–†–∞–±–æ—Ç–∞ –º–æ–¥–µ–ª–∏ –≤–∏—Å–ø–µ—Ä —Å –∑–∞–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–∏ –≤ —Å {end - start}')
    Llama_model = lama_model()
    # –†–∞–±–æ—Ç–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –õ–õ–ú –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π
    interval_llm = get_llm_interval(transcribe_video, Llama_model)
    data_full_video = get_llm_meta_info(transcribe_video, Llama_model)
    interval_merge = insert_llm_interval(interval_llm, intervals)
    os.remove(path_audio)

    data_dict = {}
    data_full = []
    video = []
    audio = []
    for i, data_interval in enumerate(interval_merge):
        start, end, source = data_interval
        # –ù–∞–∑–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ –æ—Ç—Ä–µ–∑–∫–∞
        video_name = f'short_videos/video{i + 1}.mp4'
        audio_name = f'short_audio/audio{i + 1}.mp3'
        video.append(f'video{i + 1}.mp4')
        audio.append(f'audio{i + 1}.mp3')
        # C–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—Ä–µ–∑–∫–∞ –ø–æ –≤–∏–¥–µ–æ
        video_interval_file = f'short_videos/origin_video{i + 1}.mp4'
        # #C–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∏–¥–µ–æ –ø–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º
        ffmpeg_extract_subclip(video_path, start.total_seconds(), end.total_seconds(), targetname=video_interval_file)
        # –í—ã–¥–µ–ª–µ–Ω–∏–µ –∞—É–¥–∏–æ–¥–æ—Ä–æ–∂–∫–∏
        extract_audio(video_interval_file, audio_name)
        # –ü—Ä–µ–¥–∏–∫—Ç –≤–∏—Å–ø–µ—Ä–∞
        print('–†–∞–±–æ—Ç–∞ –º–æ–¥–µ–ª–∏ –≤–∏—Å–ø–µ—Ä –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏')
        start = time.time()
        transcribe_video_short = model.transcribe(f"{audio_name}", language='ru', verbose=False, beam_size=5, best_of=5)
        end = time.time()
        print(f'–†–∞–±–æ—Ç–∞ –º–æ–¥–µ–ª–∏ –≤–∏—Å–ø–µ—Ä –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –≤ —Å {end - start}')
        # –†–∞–±–æ—Ç–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –õ–õ–ú –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π
        if len(transcribe_video_short['text']) < 200:
            data = create_dict_data(data_full_video)
        else:
            data = get_llm_meta_info(transcribe_video_short, Llama_model)
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è –¥–ª—è –ê–ü–ò
        data['transcription'] = transcribe_video_short['text']
        data['source'] = source
        data['number'] = i + 1
        data_full.append(data)
        # C–æ–∑–¥–∞–Ω–∏–µ –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ–≥–æ –æ—Ç—Ä–µ–∑–∫–∞ –ø–æ –≤–∏–¥–µ–æ
        create_viral_video(video_interval_file, video_name)
        srt_path = f'data_sub/subs{i}.srt'
        generate_subtitles(transcribe_video_short, srt_path)
        ass_path = f'data_sub/subs{i}.ass'
        mp4_path_subs = f'data_sub/video{i}_subs.mp4'
        embed_subtitles(srt_path, ass_path, video_name, mp4_path_subs)
        # –£–¥–∞–ª–µ–Ω–∏–µ –≤–∏–¥–µ–æ –ø–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º
        os.remove(video_interval_file)
        os.remove(srt_path)
        os.remove(ass_path)
        # os.remove(video_name)
    data_dict['data'] = data_full
    data_dict['video'] = video
    data_dict['audio'] = audio
    end_total = time.time()
    print(f'–†–∞–±–æ—Ç–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –≤ —Å {end_total - start_total}')
    return data_dict